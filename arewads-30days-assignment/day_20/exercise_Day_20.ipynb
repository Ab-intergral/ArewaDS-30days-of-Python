{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Level 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch the text. Status code: 404\n",
      "The ten most frequent words in 'Romeo and Juliet':\n"
     ]
    }
   ],
   "source": [
    "# Read this url and find the 10 most frequent words. romeo_and_juliet = 'http://www.gutenberg.org/files/1112/1112.txt'\n",
    "import requests\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Fetch the text from the URL\n",
    "url = 'http://www.gutenberg.org/files/1112/1112.txt'\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    text = response.text\n",
    "else:\n",
    "    print(\"Failed to fetch the text. Status code:\", response.status_code)\n",
    "    text = \"\"\n",
    "\n",
    "# Clean the text and find the most frequent words\n",
    "def get_most_common_words(text, n=10):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    word_counts = Counter(words)\n",
    "    most_common_words = word_counts.most_common(n)\n",
    "    return most_common_words\n",
    "\n",
    "most_frequent_words = get_most_common_words(text, 10)\n",
    "\n",
    "print(\"The ten most frequent words in 'Romeo and Juliet':\")\n",
    "for word, count in most_frequent_words:\n",
    "    print(f\"{word}: {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for cat weight (metric units): {'min': 3.0, 'max': 7.5, 'mean': 4.708955223880597, 'median': 4.5, 'std_dev': 1.066533799956462}\n",
      "Statistics for cat lifespan (years): {'min': 10.5, 'max': 19.0, 'mean': 13.746268656716419, 'median': 13.5, 'std_dev': 1.5844249849048053}\n",
      "\n",
      "Frequency table of country and breed:\n",
      "Egypt - Abyssinian: 1 occurrences\n",
      "Greece - Aegean: 1 occurrences\n",
      "United States - American Bobtail: 1 occurrences\n",
      "United States - American Curl: 1 occurrences\n",
      "United States - American Shorthair: 1 occurrences\n",
      "United States - American Wirehair: 1 occurrences\n",
      "United Arab Emirates - Arabian Mau: 1 occurrences\n",
      "Australia - Australian Mist: 1 occurrences\n",
      "United States - Balinese: 1 occurrences\n",
      "United States - Bambino: 1 occurrences\n",
      "United States - Bengal: 1 occurrences\n",
      "France - Birman: 1 occurrences\n",
      "United States - Bombay: 1 occurrences\n",
      "United Kingdom - British Longhair: 1 occurrences\n",
      "United Kingdom - British Shorthair: 1 occurrences\n",
      "Burma - Burmese: 1 occurrences\n",
      "United Kingdom - Burmilla: 1 occurrences\n",
      "United States - California Spangled: 1 occurrences\n",
      "United States - Chantilly-Tiffany: 1 occurrences\n",
      "France - Chartreux: 1 occurrences\n",
      "Egypt - Chausie: 1 occurrences\n",
      "United States - Cheetoh: 1 occurrences\n",
      "United States - Colorpoint Shorthair: 1 occurrences\n",
      "United Kingdom - Cornish Rex: 1 occurrences\n",
      "Canada - Cymric: 1 occurrences\n",
      "Cyprus - Cyprus: 1 occurrences\n",
      "United Kingdom - Devon Rex: 1 occurrences\n",
      "Russia - Donskoy: 1 occurrences\n",
      "China - Dragon Li: 1 occurrences\n",
      "Egypt - Egyptian Mau: 1 occurrences\n",
      "Burma - European Burmese: 1 occurrences\n",
      "United States - Exotic Shorthair: 1 occurrences\n",
      "United Kingdom - Havana Brown: 1 occurrences\n",
      "United States - Himalayan: 1 occurrences\n",
      "Japan - Japanese Bobtail: 1 occurrences\n",
      "United States - Javanese: 1 occurrences\n",
      "Thailand - Khao Manee: 1 occurrences\n",
      "Thailand - Korat: 1 occurrences\n",
      "Russia - Kurilian: 1 occurrences\n",
      "Thailand - LaPerm: 1 occurrences\n",
      "United States - Maine Coon: 1 occurrences\n",
      "United Kingdom - Malayan: 1 occurrences\n",
      "Isle of Man - Manx: 1 occurrences\n",
      "United States - Munchkin: 1 occurrences\n",
      "United States - Nebelung: 1 occurrences\n",
      "Norway - Norwegian Forest Cat: 1 occurrences\n",
      "United States - Ocicat: 1 occurrences\n",
      "United States - Oriental: 1 occurrences\n",
      "Iran (Persia) - Persian: 1 occurrences\n",
      "United States - Pixie-bob: 1 occurrences\n",
      "United States - Ragamuffin: 1 occurrences\n",
      "United States - Ragdoll: 1 occurrences\n",
      "Russia - Russian Blue: 1 occurrences\n",
      "United States - Savannah: 1 occurrences\n",
      "United Kingdom - Scottish Fold: 1 occurrences\n",
      "United States - Selkirk Rex: 1 occurrences\n",
      "Thailand - Siamese: 1 occurrences\n",
      "Russia - Siberian: 1 occurrences\n",
      "Singapore - Singapura: 1 occurrences\n",
      "United States - Snowshoe: 1 occurrences\n",
      "Somalia - Somali: 1 occurrences\n",
      "Canada - Sphynx: 1 occurrences\n",
      "Canada - Tonkinese: 1 occurrences\n",
      "United States - Toyger: 1 occurrences\n",
      "Turkey - Turkish Angora: 1 occurrences\n",
      "Turkey - Turkish Van: 1 occurrences\n",
      "United States - York Chocolate: 1 occurrences\n"
     ]
    }
   ],
   "source": [
    "# Read the cats API and cats_api = 'https://api.thecatapi.com/v1/breeds' and find :\n",
    "# the min, max, mean, median, standard deviation of cats' weight in metric units.\n",
    "# the min, max, mean, median, standard deviation of cats' lifespan in years.\n",
    "# Create a frequency table of country and breed of cats\n",
    "import requests\n",
    "import statistics\n",
    "\n",
    "cats_api = 'https://api.thecatapi.com/v1/breeds'\n",
    "\n",
    "# Fetch data from the Cat API\n",
    "response = requests.get(cats_api)\n",
    "if response.status_code == 200:\n",
    "    cat_data = response.json()\n",
    "else:\n",
    "    print(\"Failed to fetch cat data. Status code:\", response.status_code)\n",
    "    cat_data = []\n",
    "\n",
    "# Extract weight and lifespan data\n",
    "weights_metric = [cat.get('weight').get('metric') for cat in cat_data if cat.get('weight')]\n",
    "lifespans_years = [cat.get('life_span') for cat in cat_data]\n",
    "\n",
    "# Convert weight strings to numeric values (taking the average if a range is provided)\n",
    "weights_numeric = []\n",
    "for weight in weights_metric:\n",
    "    parts = weight.split('-')\n",
    "    if len(parts) == 1:\n",
    "        weights_numeric.append(float(parts[0]))\n",
    "    else:\n",
    "        weights_numeric.append((float(parts[0]) + float(parts[1])) / 2)\n",
    "\n",
    "# Convert lifespan strings to numeric values (taking the average if a range is provided)\n",
    "lifespans_numeric = []\n",
    "for lifespan in lifespans_years:\n",
    "    parts = lifespan.split('-')\n",
    "    if len(parts) == 1:\n",
    "        lifespans_numeric.append(float(parts[0]))\n",
    "    else:\n",
    "        lifespans_numeric.append((float(parts[0]) + float(parts[1])) / 2)\n",
    "\n",
    "# Calculate statistics\n",
    "weight_stats = {\n",
    "    'min': min(weights_numeric),\n",
    "    'max': max(weights_numeric),\n",
    "    'mean': statistics.mean(weights_numeric),\n",
    "    'median': statistics.median(weights_numeric),\n",
    "    'std_dev': statistics.stdev(weights_numeric)\n",
    "}\n",
    "\n",
    "lifespan_stats = {\n",
    "    'min': min(lifespans_numeric),\n",
    "    'max': max(lifespans_numeric),\n",
    "    'mean': statistics.mean(lifespans_numeric),\n",
    "    'median': statistics.median(lifespans_numeric),\n",
    "    'std_dev': statistics.stdev(lifespans_numeric)\n",
    "}\n",
    "\n",
    "# Create a frequency table of country and breed\n",
    "freq_table = {}\n",
    "for cat in cat_data:\n",
    "    country = cat.get('origin')\n",
    "    breed = cat.get('name')\n",
    "    if country and breed:\n",
    "        key = f\"{country} - {breed}\"\n",
    "        freq_table[key] = freq_table.get(key, 0) + 1\n",
    "\n",
    "# Print results\n",
    "print(\"Statistics for cat weight (metric units):\", weight_stats)\n",
    "print(\"Statistics for cat lifespan (years):\", lifespan_stats)\n",
    "print(\"\\nFrequency table of country and breed:\")\n",
    "for key, value in freq_table.items():\n",
    "    print(f\"{key}: {value} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Largest Countries:\n",
      "Russia: 17098242.0 square kilometers\n",
      "Antarctica: 14000000.0 square kilometers\n",
      "Canada: 9984670.0 square kilometers\n",
      "China: 9706961.0 square kilometers\n",
      "United States: 9372610.0 square kilometers\n",
      "Brazil: 8515767.0 square kilometers\n",
      "Australia: 7692024.0 square kilometers\n",
      "India: 3287590.0 square kilometers\n",
      "Argentina: 2780400.0 square kilometers\n",
      "Kazakhstan: 2724900.0 square kilometers\n",
      "\n",
      "10 Most Spoken Languages:\n",
      "eng: 91 countries\n",
      "fra: 46 countries\n",
      "ara: 25 countries\n",
      "spa: 24 countries\n",
      "por: 10 countries\n",
      "nld: 7 countries\n",
      "rus: 7 countries\n",
      "deu: 5 countries\n",
      "zho: 5 countries\n",
      "tsn: 4 countries\n",
      "\n",
      "Total Number of Languages: 155\n"
     ]
    }
   ],
   "source": [
    "# Read the countries API and find\n",
    "# the 10 largest countries\n",
    "# the 10 most spoken languages\n",
    "# the total number of languages in the countries API\n",
    "import requests\n",
    "\n",
    "# Fetch data from the countries API\n",
    "countries_api = 'https://restcountries.com/v3.1/all'\n",
    "response = requests.get(countries_api)\n",
    "if response.status_code == 200:\n",
    "    countries_data = response.json()\n",
    "else:\n",
    "    print(\"Failed to fetch countries data. Status code:\", response.status_code)\n",
    "    countries_data = []\n",
    "\n",
    "# Extract country sizes and languages\n",
    "country_sizes = {}\n",
    "spoken_languages = []\n",
    "\n",
    "for country in countries_data:\n",
    "    if 'name' in country and 'area' in country:\n",
    "        country_sizes[country['name']['common']] = country['area']\n",
    "\n",
    "    if 'languages' in country:\n",
    "        spoken_languages.extend(country['languages'].keys())\n",
    "\n",
    "# Calculate the 10 largest countries\n",
    "largest_countries = sorted(country_sizes.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Calculate the 10 most spoken languages\n",
    "language_count = {}\n",
    "for language in spoken_languages:\n",
    "    language_count[language] = language_count.get(language, 0) + 1\n",
    "\n",
    "most_spoken_languages = sorted(language_count.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Calculate the total number of languages\n",
    "total_languages = len(set(spoken_languages))\n",
    "\n",
    "# Display results\n",
    "print(\"10 Largest Countries:\")\n",
    "for country, size in largest_countries:\n",
    "    print(f\"{country}: {size} square kilometers\")\n",
    "\n",
    "print(\"\\n10 Most Spoken Languages:\")\n",
    "for language, count in most_spoken_languages:\n",
    "    print(f\"{language}: {count} countries\")\n",
    "\n",
    "print(f\"\\nTotal Number of Languages: {total_languages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of the page: UCI Machine Learning Repository\n",
      " Derived from simple hierarchical decision model, this database may be useful for testing constructive induction and structure discovery methods.\n",
      "Multivariate\n",
      "Other\n",
      "Classification\n",
      "Categorical\n",
      "1728\n",
      "6\n",
      "Additional Information\n",
      "Car Evaluation Database was derived from a simple hierarchical decision model originally developed for the demonstration of DEX, M. Bohanec, V. Rajkovic: Expert system for decision making. Sistemica 1(1), pp. 145-157, 1990.). The model evaluates cars according to the following concept structure:\n",
      "\n",
      "CAR                      car acceptability\n",
      ". PRICE                  overall price\n",
      ". . buying               buying price\n",
      ". . maint                price of the maintenance\n",
      ". TECH                   technical characteristics\n",
      ". . COMFORT              comfort\n",
      ". . . doors              number of doors\n",
      ". . . persons            capacity in terms of persons to carry\n",
      ". . . lug_boot           the size of luggage boot\n",
      ". . safety               estimated safety of the car\n",
      "\n",
      "Input attributes are printed in lowercase. Besides the target concept (CAR), the model includes three intermediate concepts: PRICE, TECH, COMFORT. Every concept is in the original model related to its lower level descendants by a set of examples (for these examples sets see http://www-ai.ijs.si/BlazZupan/car.html).\n",
      "\n",
      "The Car Evaluation Database contains examples with the structural information removed, i.e., directly relates CAR to the six input attributes: buying, maint, doors, persons, lug_boot, safety.\n",
      "\n",
      "Because of known underlying concept structure, this database may be particularly useful for testing constructive induction and structure discovery methods.\n",
      "\n",
      "Has Missing Values?\n",
      "No\n",
      "By M. Bohanec, V. Rajkovič. 1988\n",
      "Published in 8th Intl Workshop on Expert Systems and their Applications, Avignon, France\n",
      "0 to 7 of 7\n",
      "\n",
      "buying:   vhigh, high, med, low.\n",
      "maint:    vhigh, high, med, low.\n",
      "doors:    2, 3, 4, 5more.\n",
      "persons:  2, 4, more.\n",
      "lug_boot: small, med, big.\n",
      "safety:   low, med, high.\n",
      "Class Labels\n",
      "unacc, acc, good, vgood\n",
      "By Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Danielle Maddix, Syama Rangapuram, David Salinas, Jasper Schulz, Lorenzo Stella, Ali Turkmen, Yuyang Wang. 2019\n",
      "Published in ArXiv. \n",
      "By Xiyang Hu, Cynthia Rudin, Margo Seltzer. 2019\n",
      "Published in ArXiv. \n",
      "By Eric Keiji, Gabriel Ferreira, Claudio Silva, Roberto Cesar. 2017\n",
      "Published in ArXiv. \n",
      "By Vahid Jalali, David Leake, Najmeh Forouzandehmehr. 2017\n",
      "Published in IJCAI. \n",
      "By Kedar Potdar, Taher Pardawala, Chinmay Pai. 2017\n",
      "Published in International Journal of Computer Applications. \n",
      "0 to 5 of 34\n",
      "There are no reviews for this dataset yet.\n",
      "Import in Python\n",
      "Marko Bohanec\n",
      "This dataset is licensed under a\n",
      "Creative Commons Attribution 4.0 International\n",
      "(CC BY 4.0) license.\n",
      "This allows for the sharing and adaptation of the datasets for any purpose,\n",
      "provided that the appropriate credit is given.\n",
      "By using the UCI Machine Learning Repository,\n",
      "you acknowledge and accept the cookies and privacy practices used by the UCI Machine Learning Repository.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the dataset page\n",
    "url = 'https://archive.ics.uci.edu/ml/datasets/Car+Evaluation'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find and print the main content of the page (for example, the title)\n",
    "    title = soup.find('title')\n",
    "    if title:\n",
    "        print(\"Title of the page:\", title.text)\n",
    "    else:\n",
    "        print(\"Title not found.\")\n",
    "\n",
    "    # You can find and extract other elements of the page using BeautifulSoup methods\n",
    "    # For example, finding all paragraph elements\n",
    "    paragraphs = soup.find_all('p')\n",
    "    for paragraph in paragraphs:\n",
    "        print(paragraph.text)\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
